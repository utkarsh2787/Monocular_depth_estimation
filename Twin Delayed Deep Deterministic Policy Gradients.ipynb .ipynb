{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Twin Delayed Deep Deterministic Policy Gradients.ipynb ","provenance":[],"authorship_tag":"ABX9TyPo5M8a2SvQAG0pWItyPzHY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"cxLpVQjkJUdp","executionInfo":{"status":"ok","timestamp":1655734164051,"user_tz":-330,"elapsed":4487,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"outputs":[],"source":["## Setup \n","import numpy as np \n","import pandas as pd \n","\n","import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import os  "]},{"cell_type":"code","source":["## Create a buffer of memories that we can sample uniformly \n","class ReplayBuffer() :\n","    \n","    ## Dealing with the number of commponents of an action as action space is continuous \n","    def __init__(self, max_size, input_shape, n_actions) : \n","        self.mem_size = max_size\n","\n","        ## Saves the counter of the first unsaved memory so that we can insert a new memory there. \n","        self.mem_cntr = 0\n","        self.state_memory = np.zeros((self.mem_size, *input_shape))\n","        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n","        self.action_memory = np.zeros((self.mem_size, n_actions))\n","        self.reward_memory = np.zeros(self.mem_size)\n","        self.terminal_memory = np.zeros(self.mem_size, dtype = np.bool)\n","\n","    ## An interface to store new memories\n","    def store_transition(self, state, action, reward, state_, done) :\n","        index = self.mem_cntr % self.mem_size\n","        self.state_memory[index] = state \n","        self.new_state_memory[index] = state_\n","        self.reward_memory[index] = reward\n","        self.action_memory[index] = action \n","\n","        self.mem_cntr += 1\n","\n","    ## Sampling the Buffer Memory \n","    def sample_buffer(self, batch_size) :\n","        max_mem = min(self.mem_cntr, self.mem_size)\n","        batch = np.random.choice(max_mem, batch_size)\n","        \n","        states = self.state_memory[batch]\n","        states_ = self.new_state_memory[batch]\n","        actions = self.action_memory[batch]\n","        rewards = self.reward_memory[batch]\n","        dones = self.terminal_memory[batch]\n","\n","        return states, actions, rewards, states_, dones \n"],"metadata":{"id":"nr2QHY_IKOaM","executionInfo":{"status":"ok","timestamp":1655734164053,"user_tz":-330,"elapsed":11,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["## Critic Network Class \n","class CriticNetwork(nn.Module) :\n","    \n","    def __init__(self, beta, input_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir = \"tmp/td3\") :\n","\n","        super(CriticNetwork, self).__init__()\n","        self.input_dims = input_dims \n","        self.fc1_dims = fc1_dims\n","        self.fc2_dims = fc2_dims \n","        self.n_actions = n_actions \n","        self.name = name \n","        self.checkpoint_dir = chkpt_dir \n","        self.checkpoint_file = os.apth.join(self.checkpoint_dir, name + \"_td3\")\n","\n","        self.fc1 = nn.Linear(self.input_dims[0] + n_actions, self.fc1_dims)\n","        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n","        self.q1 = nn.Linear(self.fc2_dims, 1)\n","\n","        self.optimizer = optim.Adam(self.parameters(), lr = beta)\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","        self.to(self.device)\n","\n","    def forward(self, state, action) :\n","         q1_action_value = self.fc1(torch.cat([state, action], dim = 1))\n","         q1_action_value = F.relu(q1_action_value)\n","         q1_action_value = self.fc2(q1_action_value)\n","         q1_action_value = F.relt(q1_action_value)\n","\n","         q1 = self.q1(q1_action_value)\n","         return q1\n","\n","    def save_checkpoint(self) :\n","        print(\"... Saving Checkpoint ...\")\n","        torch.save(self.state_dict(), self.checkpoint_file)\n","\n","    def load_checkpoint(self) :\n","        print(\"... Loading Checkpoint ...\")\n","        torch.load_state_dict(torch.load(self.checkpoint_file))"],"metadata":{"id":"dcqMkB2hNpbn","executionInfo":{"status":"ok","timestamp":1655734164053,"user_tz":-330,"elapsed":10,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["## Actor Network \n","\n","class ActorNetwork(nn.Module) :\n","    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir = \"tmp/td3\") :\n","\n","        super(ActorNetwork, self).__init__()\n","        self.input_dims = input_dims \n","        self.fc1_dims = fc1_dims \n","        self.fc2_dims = fc2_dims\n","        self.n_actions = n_actions \n","        self.name = name \n","        self.checkpoint_dir = chkpt_dir\n","        self.checkpoint_file = os.path.join(self.checkpoint_dir, name + \"_td3\")\n","\n","        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n","        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n","        self.mu = nn.Linear(self.fc2_dims, n_actions)\n","\n","        self.optimizer = optim.Adam(self.parameters(), lr = alpha)\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","        self.to(self.device)\n","\n","    def forward(self, state) :\n","        prob = self.fc1(state)\n","        prob = F.relu(prob)\n","        prob = self.fc2(state)\n","        prob = F.relu(prob)\n","\n","        prob = torch.tanh(self.mu(prob))\n","        return prob\n","\n","    def save_checkpoint(self) :\n","        print(\"... Saving Checkpoint ...\")\n","        torch.save(self.state_dict(), self.checkpoint_file)\n","\n","    def load_checkpoint(self) :\n","        print(\"... Loading Checkpoint ...\")\n","        torch.load_state_dict(torch.load(self.checkpoint_file))\n","\n"],"metadata":{"id":"jngzDt9BRDq9","executionInfo":{"status":"ok","timestamp":1655734164055,"user_tz":-330,"elapsed":11,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class Agent() :\n","\n","    def __init__(self, alpha, beta, input_dims, tau, env, gamma = 0.99, update_actor_interval = 2, warmup = 1000, n_actions = 2, max_size = 1e6, \n","                 layer1_size = 400, layer2_size = 300, batch_size = 100, noise = 0.1)  :\n","\n","        self.gamma = gamma \n","        self.tau = tau\n","        self.max_actions = env.action_space.high\n","        self.min_actions = env.action_space.low\n","        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n","        self.batch_size = batch_size\n","        self.learn_step_cntr = 0\n","        self.time_step = 0\n","        self.warmup = warmup \n","        self.n_actions = n_actions \n","        self.update_actor_iter = update_actor_interval\n","\n","        self.actor = ActorNetwork(alpha, input_dims, layer1_size, layer2_size, \n","                                  n_actions, name = \"actor\")\n","        \n","        self.critic_1 = CriticNetwork(beta, input_dims, layer1_size, layer2_size,\n","                                      n_actions, name = \"critic_1\")\n","        \n","        self.critic_2 = CriticNetwork(beta, input_dims, layer1_size, layer2_size,\n","                                      n_actions, name = \"critic_2\")\n","\n","        ## Initating the target network, \n","        ## we do not need to perform back propagation on this network\n","        ## we only need the parameters\n","        ## so, the learning rate is irrelevant        \n","        self.target_actor = ActorNetwork(alpha, input_dims, layer1_size, layer2_size, \n","                                         n_actions, name = \"target_actor\")\n","        \n","        self.target_critic_1 = CriticNetwork(beta, input_dims, layer1_size, layer2_size,\n","                                      n_actions, name = \"target_critic_1\")\n","        \n","        self.target_critic_2 = CriticNetwork(beta, input_dims, layer1_size, layer2_size,\n","                                      n_actions, name = \"target_critic_2\")\n","        \n","        self.noise = noise\n","        self.update_network_parameters(tau = 1)\n","\n","    def choose_action(self, observation) :\n","        if self.time_step < self.warmup :\n","            mu = torch.tensor(np.random.normal(scale = self.noise, size = (self.n_actions, )))\n","\n","        else :\n","            state = torch.tensor(observation, dtype = torch.float).to(self.actor.device)\n","            mu = self.actor.forward(state).to(self.actor.device)\n","\n","        mu_prime = mu + torch.tensor(np.random.normal(scale = self.noise), dtype = torch.float).to(self.actor.device)\n","\n","        mu_prime = torch.clamp(mu_prime, self.min_action[0], self.max_action[0])\n","        self.time_step += 1\n","\n","        return mu_prime.cpu().detach.numpy()\n","\n","    def remember(self, state, action, reward, new_state, done) :\n","        self.memory.store_transition(state, action, reward, new_state, done)\n","\n","    def learn(self) :\n","        if self.memory.mem_cntr < self.batch_size :\n","            return \n","\n","        state, action, reward, new_state, done = \\\n","                        self.memory.sample_buffer(self.batch_size)\n","\n","        reward = torch.tensor(reward, dtype = torch.float).to(self.critic_1.device)\n","        done = torch.tensor(done).to(self.critic_1.device)\n","        state_ = torch.tensor(new_state, dtype = torch.float).to(self.critic_1.device)\n","        state = torch.tensor(state, dtype = torch.float).to(self.critic_1.device)\n","        action = torch.tensor(action, dtype = torch.float).to(self.critic_1.device)\n","\n","        target_actions = self.target_actor.forward(state_)\n","        target_actions = target_actions + \\\n","                    torch.clamp(torch.tensor(np.random.normal(scale = 0.2)), -0.5, 0.5)\n","        target_actions = torch.clamp(target_actions, self.min_action[0], self.max_action[0])\n","\n","        q1_ = self.target_critic_1.forward(state_, target_actions)\n","        q2_ = self.target_critic_2.forward(state_, target_actions)\n","\n","        q1 = self.critic_1.forward(state, action)\n","        q2 = self.critic_2.forward(state, action)\n","\n","        q1_[done] = 0.0\n","        q2_[done] = 0.0\n","\n","        q1_ = q1_.view(-1)\n","        q2_ = q2_.view(-1)\n","\n","        critic_value_ = torch.min(q1_, q2_)\n","\n","        target = reward + self.gamma * critic_value_\n","        target = target.view(self.batch_size, 1)\n","\n","        self.critic_1.optimizer.zero_grad()\n","        self.critic_2.optimizer.zero_grad()\n","\n","        q1_loss = F.mse_loss(target, q1)\n","        q2_loss = F.mse_loss(target, q2)\n","        critic_loss = q1_loss + q2_loss\n","        critic_loss.backward()\n","        self.critic_1.optimizer.step()\n","        self.critic_2.optimizer.step()\n","\n","        self.learn_step_cntr += 1\n","\n","        if self.learn_step_cntr % self.update_actor_iter != 0 :\n","            return \n","\n","        self.actor.optimizer.zero_grad()\n","        actor_q1_loss = self.critic_1.forward(state, self.actor.forward(state))\n","        actor_loss = -torch.mean(actor_q1_loss)\n","        actor_loss.backward()\n","        self.actor.optimizer.step()\n","\n","        self.update_network_parameters()\n","\n","    def update_network_parameters(self, tau = None) :\n","        \n","        if tau is None :\n","            tau = self.tau\n","\n","        actor_params = self.actor.named_parameters()\n","        critic_1_params = self.critic_1.named_parameters()\n","        critic_2_params = self.critic_2.named_parameters()\n","        target_actor_params = self.target_actor.named_parameters()\n","        target_critic_1_params = self.target_critic_1.named_parameters()\n","        target_critic_2_params = self.target_critic_2.named_parameters() \n","\n","        critic_1 = dict(critic_1_params)\n","        critic_2 = dict(critic_2_params)\n","        actor = dict(actor_params)\n","        target_actor = dict(target_actor_params)\n","        target_critic_1 = dict(target_critic_1_params)\n","        target_critic_2 = dict(target_critic_2_params)\n","\n","        for name in critic_1 :\n","            critic_1[name] = tau * critic_1[name].clone() + (1 - tau) * target_critic_1[name].clone()\n","\n","        for name in critic_2 :\n","            critic_2[name] = tau * critic_2[name].clone() + (1 - tau) * target_critic_2[name].clone()\n","\n","        for name in actor :\n","            actor[name] = tau * actor[name].clone() + (1 - tau) * target_actor[name].clone()\n","\n","        self.target_critic_1.load_state_dict(critic_1)\n","        self.target_critic_2.load_state_dict(critic_2)\n","        self.target_actor.load_state_dict(actor)\n","\n","    def save_models(self):\n","        self.actor.save_checkpoint()\n","        self.target_actor.save_checkpoint()\n","        self.critic_1.save_checkpoint()\n","        self.critic_2.save_checkpoint()\n","        self.target_critic_1.save_checkpoint()\n","        self.target_critic_2.save_checkpoint()\n","\n","    def load_models(self):\n","        self.actor.load_checkpoint()\n","        self.target_actor.load_checkpoint()\n","        self.critic_1.load_checkpoint()\n","        self.critic_2.load_checkpoint()\n","        self.target_critic_1.load_checkpoint()\n","        self.target_critic_2.load_checkpoint()\n","\n","                "],"metadata":{"id":"2rToMluI5Qh6","executionInfo":{"status":"ok","timestamp":1655735771499,"user_tz":-330,"elapsed":406,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"caKmj2H4cf1Z"},"execution_count":null,"outputs":[]}]}