{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Monocular Depth Estimation using Adaptive Geometric Attention.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMvn6JkZS7Fursc2pIM5aVd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Monocular Depth Estimation with Adaptive Geometric Attention**\n","\n","This work was most recently published and proposed an encoder - decoder based model by extracting the similarity in the depth map and the normal single RGB Image at the Geometric Edges. They used a concepth of cosine similarity in their Attention modeule - a concept borrowed from the field of Natural Language Processing. "],"metadata":{"id":"Ypco3NJaaG1c"}},{"cell_type":"markdown","source":["# **Setup And Imports**"],"metadata":{"id":"2TspzoKjbjFh"}},{"cell_type":"code","source":["!pip install albumentations==0.4.6"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZg-nr4FK9E9","executionInfo":{"status":"ok","timestamp":1656407875050,"user_tz":-330,"elapsed":6739,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}},"outputId":"e61429c7-7244-4f62-e0ee-8f3ec204abd1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: albumentations==0.4.6 in /usr/local/lib/python3.7/dist-packages (0.4.6)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n","Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (0.4.0)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.3.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.1.1)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"CIBMtNsvZ90T","executionInfo":{"status":"ok","timestamp":1656407876630,"user_tz":-330,"elapsed":1585,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"outputs":[],"source":["import os \n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt \n","import cv2 \n","import PIL \n","\n","import torch\n","from torch import nn \n","from torch import optim as O\n","from torch.nn import Module\n","from torch.nn import functional as f\n","from torch.nn.modules.activation import Sigmoid\n","from torch.nn.modules.upsampling import UpsamplingBilinear2d\n","from torch.nn.modules.conv import Conv2d\n","from torch.optim.lr_scheduler import LinearLR\n","from torch.utils.data import Dataset \n","from torch.utils.data import DataLoader\n","\n","import torchvision\n","from torchvision import utils as Vision_utils\n","from torchvision import models\n","from torchvision.transforms import functional as TF\n","\n","from torchsummary import summary\n","  \n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from tqdm import tqdm \n","\n","from warnings import filterwarnings as IF"]},{"cell_type":"code","source":["IF(\"ignore\")"],"metadata":{"id":"qMiBdtRqVbyI","executionInfo":{"status":"ok","timestamp":1656407876631,"user_tz":-330,"elapsed":14,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# **Loading the Dataset**\n","\n","The dataset we are using currently is the NYU depth -v2 dataset which was downloaded from kaggle and stored in an external hard drive. There is a .csv file that has thw path to each RGB image and its corresponding grayscale DepthMap."],"metadata":{"id":"pUgqytEcLWf5"}},{"cell_type":"code","source":["## Mounting the Gooogle Drive on the Colab file\n","from google.colab import drive \n","drive.mount('/content/drive')"],"metadata":{"id":"gqEg1zcOKJzS","executionInfo":{"status":"ok","timestamp":1656407880612,"user_tz":-330,"elapsed":3994,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a899ec31-4ff6-4260-cb0d-0405f05b9e9d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["## Loading the .csv file \n","df = pd.read_csv(\"/content/drive/MyDrive/NYU-depthV2/nyu_data/data/nyu2_train.csv\", names = [\"image\", \"depth\"])\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"tf59vgRiCk9A","executionInfo":{"status":"ok","timestamp":1656407880612,"user_tz":-330,"elapsed":42,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}},"outputId":"a8b38558-91ac-4a57-cb16-5b790cff5356"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                          image  \\\n","0   data/nyu2_train/living_room_0038_out/37.jpg   \n","1  data/nyu2_train/living_room_0038_out/115.jpg   \n","2    data/nyu2_train/living_room_0038_out/6.jpg   \n","3   data/nyu2_train/living_room_0038_out/49.jpg   \n","4  data/nyu2_train/living_room_0038_out/152.jpg   \n","\n","                                          depth  \n","0   data/nyu2_train/living_room_0038_out/37.png  \n","1  data/nyu2_train/living_room_0038_out/115.png  \n","2    data/nyu2_train/living_room_0038_out/6.png  \n","3   data/nyu2_train/living_room_0038_out/49.png  \n","4  data/nyu2_train/living_room_0038_out/152.png  "],"text/html":["\n","  <div id=\"df-83466403-48cd-4252-9674-55269a7bd382\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>depth</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>data/nyu2_train/living_room_0038_out/37.jpg</td>\n","      <td>data/nyu2_train/living_room_0038_out/37.png</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>data/nyu2_train/living_room_0038_out/115.jpg</td>\n","      <td>data/nyu2_train/living_room_0038_out/115.png</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>data/nyu2_train/living_room_0038_out/6.jpg</td>\n","      <td>data/nyu2_train/living_room_0038_out/6.png</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>data/nyu2_train/living_room_0038_out/49.jpg</td>\n","      <td>data/nyu2_train/living_room_0038_out/49.png</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>data/nyu2_train/living_room_0038_out/152.jpg</td>\n","      <td>data/nyu2_train/living_room_0038_out/152.png</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83466403-48cd-4252-9674-55269a7bd382')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-83466403-48cd-4252-9674-55269a7bd382 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-83466403-48cd-4252-9674-55269a7bd382');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["## Setting up the Path root \n","path_root = \"/content/drive/MyDrive/NYU-depthV2/nyu_data/\"\n","df[\"image\"] = df[\"image\"].apply(lambda x : path_root + str(x))\n","df[\"depth\"] = df[\"depth\"].apply(lambda x : path_root + str(x))\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"V-BJ5xQ3DruW","executionInfo":{"status":"ok","timestamp":1656407880613,"user_tz":-330,"elapsed":37,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}},"outputId":"8daf76e5-2cc5-4625-fbde-151839f487d0"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               image  \\\n","0  /content/drive/MyDrive/NYU-depthV2/nyu_data/da...   \n","1  /content/drive/MyDrive/NYU-depthV2/nyu_data/da...   \n","2  /content/drive/MyDrive/NYU-depthV2/nyu_data/da...   \n","3  /content/drive/MyDrive/NYU-depthV2/nyu_data/da...   \n","4  /content/drive/MyDrive/NYU-depthV2/nyu_data/da...   \n","\n","                                               depth  \n","0  /content/drive/MyDrive/NYU-depthV2/nyu_data/da...  \n","1  /content/drive/MyDrive/NYU-depthV2/nyu_data/da...  \n","2  /content/drive/MyDrive/NYU-depthV2/nyu_data/da...  \n","3  /content/drive/MyDrive/NYU-depthV2/nyu_data/da...  \n","4  /content/drive/MyDrive/NYU-depthV2/nyu_data/da...  "],"text/html":["\n","  <div id=\"df-4f186175-7c75-4361-9076-137377b8f65e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>depth</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/MyDrive/NYU-depthV2/nyu_data/da...</td>\n","      <td>/content/drive/MyDrive/NYU-depthV2/nyu_data/da...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/MyDrive/NYU-depthV2/nyu_data/da...</td>\n","      <td>/content/drive/MyDrive/NYU-depthV2/nyu_data/da...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/MyDrive/NYU-depthV2/nyu_data/da...</td>\n","      <td>/content/drive/MyDrive/NYU-depthV2/nyu_data/da...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/MyDrive/NYU-depthV2/nyu_data/da...</td>\n","      <td>/content/drive/MyDrive/NYU-depthV2/nyu_data/da...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/MyDrive/NYU-depthV2/nyu_data/da...</td>\n","      <td>/content/drive/MyDrive/NYU-depthV2/nyu_data/da...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f186175-7c75-4361-9076-137377b8f65e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4f186175-7c75-4361-9076-137377b8f65e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4f186175-7c75-4361-9076-137377b8f65e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["## Splitting the training and validation dataset\n","train_df = df[0 : 40550]\n","val_df = df[40550 : ]\n","del df"],"metadata":{"id":"D4VloXyeD6lc","executionInfo":{"status":"ok","timestamp":1656407880614,"user_tz":-330,"elapsed":36,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# **Building the Data Pipeline**\n","\n","1. The Pipeline takes the path dataframe and returns RGB images and corresponding Depth maps one batch at a time.\n","2. It will reshape and resize the images according to our hyperparameters as well.\n","3. To implement Image Augmentations we are going to use the Albumentations library which will automatically take care of the resizing part of the problem. "],"metadata":{"id":"DhYTMnE6Motn"}},{"cell_type":"code","source":["class DepthDataset(Dataset) :\n","\n","    def __init__(self, df, transform = None) :\n","\n","        self.img_dir = df[\"image\"]\n","        self.depth_dir = df[\"depth\"]\n","        self.transform = transform \n","\n","    def __len__(self) :\n","        return len(self.img_dir)\n","\n","    def __getitem__(self, index) :\n","        img_path = self.img_dir[index]\n","        depth_path = self.depth_dir[index]\n","\n","        image_ = cv2.imread(img_path)\n","        image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n","        image_ = np.array(image_, dtype = np.float32)\n","\n","        depth_ = cv2.imread(depth_path)\n","        depth_ = cv2.cvtColor(depth_, cv2.COLOR_BGR2GRAY)\n","        depth_ = np.array(depth_, dtype = np.float32)\n","        shape = depth_.shape\n","        depth_.resize(shape[0], shape[1], 1)\n","\n","        if self.transform is not None :\n","            augmentations = self.transform(image = image_, mask = depth_)\n","            image_ = augmentations[\"image\"]\n","            depth_ = augmentations[\"mask\"]\n","\n","\n","        return image_, depth_\n","\n"],"metadata":{"id":"erxCqt02NMkO","executionInfo":{"status":"ok","timestamp":1656407880616,"user_tz":-330,"elapsed":37,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["## Now we are going to use the DataLoader Functionality in the Pytorch Library \n","## So we will define a function get_loaders that will return the Dataloader object \n","## For both training and Validation dataset according to the Hyperparameters\n","\n","def get_loaders(\n","    train_df, val_df, batch_size, train_transform, val_transform, num_workers = 4, pin_memory = True\n",") :\n","\n","    train_DS = DepthDataset(train_df, train_transform)\n","    val_DS = DepthDataset(val_df, val_transform)\n","\n","    train_loader = DataLoader(\n","        train_DS, batch_size = batch_size, num_workers = num_workers, pin_memory = pin_memory, shuffle = True \n","    )\n","    \n","    val_loader = DataLoader(\n","        val_DS, batch_size = batch_size, num_workers = num_workers, pin_memory = pin_memory, shuffle = False\n","    )\n","\n","    return train_loader, val_loader"],"metadata":{"id":"i_ZqmLSCdKGM","executionInfo":{"status":"ok","timestamp":1656407880616,"user_tz":-330,"elapsed":37,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# **Other Utility Functions**\n","\n","In this section we implemented other important Utility Functions like saving model checkpoint, loading model checkpoint, saving predictions as images etc. "],"metadata":{"id":"nH6IUqNzgN4m"}},{"cell_type":"code","source":["def save_checkpoint(state, filename = \"my_checkpoint.pth.tar\") :\n","    print(\"=> Saving Checkpoint\")\n","    torch.save(state, filename)\n","\n","def load_checkpoint(checkpoint, model) :\n","    print(\"=> Loading Checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","\n","## To ckeck Accuracy we are going to use  the DICE score\n","\n","def check_accuracy(loader, model, device = \"cuda\") :\n","    num_correct = 0\n","    num_pixels = 0\n","    dice_score = 0\n","    model.eval()\n","\n","    with torch.no_grad() :\n","        for x, y in loader :\n","            x = x.to(device)\n","            y = y.to(device).unsqueeze(1)\n","            preds = model(x)\n","            num_correct += (preds == y).sum()\n","            num_pixels += torch.numel(preds)\n","            dice_score += (2 * (preds * y).sum()) / (\n","                (preds + y).sum() + 1e-8\n","            )\n","\n","    print(\n","        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n","    )\n","    print(f\"Dice score: {dice_score/len(loader)}\")\n","    model.train()\n","    return\n","\n","\n","\"\"\"\n","\n","def save_predictions_as_imgs(loader, model, folder = \"saved_images/\", device = \"cuda\") :\n","\n","    model.eval()\n","    for idx, (x, y) in enumerate(loader) :\n","        x = x.to(device = device)\n","        with torch.no_grad() :\n","\n","\"\"\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"mG1QBue4gM4O","executionInfo":{"status":"ok","timestamp":1656407880617,"user_tz":-330,"elapsed":37,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}},"outputId":"a5018c68-2cd0-4474-b8a0-158a1912ba80"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\ndef save_predictions_as_imgs(loader, model, folder = \"saved_images/\", device = \"cuda\") :\\n\\n    model.eval()\\n    for idx, (x, y) in enumerate(loader) :\\n        x = x.to(device = device)\\n        with torch.no_grad() :\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["# **Preparing the Hyperparameters**"],"metadata":{"id":"zRrGw6ol8fLC"}},{"cell_type":"code","source":["learning_rate = 3e-4\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","batch_size = 16\n","num_epochs = 500\n","Image_height = 224\n","Image_width = 224\n","pin_memory = True \n","load_model = False\n","num_workers = 2"],"metadata":{"id":"16fJ34-j8SfM","executionInfo":{"status":"ok","timestamp":1656407880618,"user_tz":-330,"elapsed":37,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# **Building the Model**\n","\n","Now we begin to create the Model. the research paper defines a new Module named as Adaptive Geometric Attention (AGA) which will be created from scratch. For the Encoder we will use a pre-trained ResNeXt (101 - 32d) with Image Net. Also we will declare modules Atrous Spatial Pyramid Pooling (ASPP) and Dilated Residual Blocks (DRBs) and arranging the neccessary skip connections to  fiinally complete the model."],"metadata":{"id":"SKnz38yYirhA"}},{"cell_type":"markdown","source":["The Next Three Cells are used to declare the three special modules namely the Adaptive Geometric Attention (AGA Module), Atrous Spatial Pyramid Pooling (ASPP) and the Dilated Residual Block (DRB) "],"metadata":{"id":"ZPJxrPDg4a27"}},{"cell_type":"code","source":["class AGA_Block(Module) :\n","\n","    def __init__(self, C, H, W) :\n","        super(AGA_Block, self).__init__()\n","        self.channel_wise_attention = nn.Sequential(\n","            nn.AvgPool2d((H, W), 1), \n","            nn.Conv2d(2*C, C, (1, 1), (1, 1), padding = \"same\", bias = False), \n","            nn.ReLU(),\n","            nn.Conv2d(C, C, (1, 1), (1, 1), padding = \"same\", bias = False), \n","            nn.Sigmoid()\n","        )\n","\n","        self.convLow_1 = nn.Conv2d(C, C, (1, 1), (1, 1), padding = \"same\", bias = False)\n","        self.convHigh_1 = nn.Conv2d(C, C, (1, 1), (1, 1), padding = \"same\", bias = False)\n","\n","        self.convLow_2 = nn.Conv2d(C, C, (1, 1), (1, 1), padding = \"same\", bias = False)\n","        self.convHigh_2 = nn.Conv2d(C, C, (1, 1), (1, 1), padding = \"same\", bias = False)\n","\n","    def forward(self, low_x, high_x) :\n","\n","        ## The Forward part is of three sections \n","        ## Section 1 : Channel - Wise Attention Maps\n","        x_ca = torch.cat((low_x, high_x), dim = 1)\n","        x_ca = self.channel_wise_attention(x_ca)\n","        ## The Above has taken two input tensors of size [N, C, H, w]\n","        ## Returns a Tensor of Size [N, C, 1, 1]\n","\n","        ## Section 2 : Spatial Attention Map 1\n","        x_low_1 = self.convLow_1(low_x)\n","        x_low_1 = f.normalize(x_low_1, p = 2, dim = 1)\n","        x_high_1 = self.convHigh_1(high_x)\n","        x_high_1 = f.normalize(x_high_1, p = 2, dim = 1)\n","        x_comb_1 = torch.mul(x_low_1, x_high_1)\n","        x_comb_1 = torch.sum(x_comb_1, dim = 1)\n","        shape_1 = x_comb_1.shape\n","        x_comb_1 = x_comb_1.reshape(shape_1[0], 1, shape_1[1], shape_1[2])\n","        sa_1 = torch.abs(x_comb_1)\n","        ## The Dimensions of the Tensor SA-1 is [N, 1, H, W]\n","\n","        ## Deleting unused Tensors as they are no longer needed to free up RAM space \n","        del x_low_1, x_high_1, x_comb_1, shape_1\n","\n","        ## Section 3 : Spatial Attention Map 2\n","        x_low_2 = self.convLow_2(low_x)\n","        x_low_2 = f.normalize(x_low_2, p = 2, dim = 1)\n","        x_high_2 = self.convHigh_2(high_x)\n","        x_high_2 = f.normalize(x_high_2, p = 2, dim = 1)\n","        x_comb_2 = torch.mul(x_low_2, x_high_2)\n","        x_comb_2 = torch.sum(x_comb_2, dim = 1)\n","        shape_2 = x_comb_2.shape\n","        x_comb_2 = x_comb_2.reshape(shape_2[0], 1, shape_2[1], shape_2[2])\n","        sa_2 = torch.abs(x_comb_2)\n","        ## The Dimensions of the Tensor SA-2 is [N, 1, H, W]\n","\n","        ## Deleting unused Tensors as they are no longer neede to free up RAM space\n","        del x_low_2, x_high_2, x_comb_2, shape_2\n","\n","        ## The two sensitivity functions\n","        ## f(x) = x and f(x) = x * (e ^ x)\n","        ## The first does not enhances the sensitivity\n","        ## The authors believe that enhancing the sensitivity of one map helps \n","        ## So According to Paper Terminology, we choose \n","        ## f1(x) = x\n","        ## f2(x) = x * (e ^ x)\n","\n","        ## Implementing f2(x)\n","        temp = torch.exp(sa_2)\n","        sa_2 = torch.mul(temp, sa_2)\n","        final_l = torch.mul(sa_2, low_x)\n","        final_l = torch.mul(final_l, x_ca)\n","        del temp, sa_2\n","\n","        ## There is no need to do anything for f1(x) as it is identity function\n","        final_h = torch.add(sa_1, high_x)\n","        del sa_1\n","\n","        ## Final Summation\n","        output = torch.add(final_l, final_h, alpha = 1)\n","        return output "],"metadata":{"id":"A0fEWuOPlQOR","executionInfo":{"status":"ok","timestamp":1656407880618,"user_tz":-330,"elapsed":36,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class ASPP_Block(Module) :\n","\n","    def __init__(self, C, H, W) :\n","        super(ASPP_Block, self).__init__()\n","        self.ImageLevelPooling = nn.Sequential(\n","            nn.AvgPool2d((H, W), 1),\n","            nn.Conv2d(C, C//5, (1, 1), (1, 1), padding = \"same\", bias = False),\n","            nn.UpsamplingBilinear2d((H, W))\n","        )\n","\n","        self.conv1 = nn.Conv2d(C, C//5, (1, 1), (1, 1), padding = \"same\", bias = False)\n","        self.conv2 = nn.Conv2d(C, C//5, (3, 3), (1, 1), padding = \"same\", bias = False, dilation = 3)\n","        self.conv3 = nn.Conv2d(C, C//5, (3, 3), (1, 1), padding = \"same\", bias = False, dilation = 6)\n","        self.conv4 = nn.Conv2d(C, C//5, (3, 3), (1, 1), padding = \"same\", bias = False, dilation = 9)\n","        self.final_conv = nn.Conv2d((C//5) * 5, C, (3, 3), (1, 1), padding = \"same\", bias = False)\n","\n","    def forward(self, x) :\n","        a = self.conv1(x)\n","        b = self.conv2(x)\n","        c = self.conv3(x)\n","        d = self.conv4(x) \n","        e = self.ImageLevelPooling(x)\n","\n","        out = torch.cat([a, b, c, d, e], dim = 1)\n","        del a, b, c, d, e\n","        out = self.final_conv(out)\n","        return out \n","    \n"],"metadata":{"id":"rIRnmoiKKBjq","executionInfo":{"status":"ok","timestamp":1656407880619,"user_tz":-330,"elapsed":36,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class DRB(Module) :\n","\n","    def __init__(self, C, H, W) :\n","        super(DRB, self).__init__()\n","        self.conv1 = nn.Conv2d(C, C, (1, 1), (1, 1), padding = \"same\", bias = False, dilation = 1)\n","        self.conv2 = nn.Conv2d(C, C, (3, 3), (1, 1), padding = \"same\", bias = False, dilation = 2)\n","        self.conv3 = nn.Conv2d(C, C, (3, 3), (1, 1), padding = \"same\", bias = False, dilation = 4)\n","        self.conv4 = nn.Conv2d(C, C, (3, 3), (1, 1), padding = \"same\", bias = False, dilation = 8)\n","        self.conv5 = nn.Conv2d(C, C, (3, 3), (1, 1), padding = \"same\", bias = False, dilation = 16)\n","\n","        self.final_conv = nn.Conv2d(5*C, C, (1, 1), (1, 1), padding = \"same\", bias = False)\n","\n","    def forward(self, x) :\n","        a = self.conv1(x)\n","        b = self.conv2(a)\n","        c = self.conv3(b)\n","        d = self.conv4(c)\n","        e = self.conv5(d)\n","\n","        out = torch.cat([a, b, c, d, e], dim = 1)\n","\n","        del a, b, c, d, e\n","        out = self.final_conv(out)\n","        return out"],"metadata":{"id":"D3Ew727s0dqy","executionInfo":{"status":"ok","timestamp":1656407880620,"user_tz":-330,"elapsed":37,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class Decoder_Layer(Module) :\n","\n","    def __init__(self, C_in, H_in, W_in, C_out, H_out, W_out) :\n","        super(Decoder_Layer, self).__init__()\n","        self.AGA = AGA_Block(C_in, H_in, W_in) \n","        self.block = nn.Sequential(\n","            DRB(C_in, H_in, W_in), \n","            nn.Conv2d(C_in, C_out, (1, 1), (1, 1), padding = \"same\", bias = False), \n","            nn.UpsamplingBilinear2d((H_out, W_out))\n","        )\n","\n","    def forward(self, low_x, high_x) :\n","        out = self.AGA(low_x, high_x)\n","        out = self.block(out)\n","        return out\n"],"metadata":{"id":"WLbqjkCdkByB","executionInfo":{"status":"ok","timestamp":1656407880621,"user_tz":-330,"elapsed":37,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["Now, we will create the final Model that will use the ResNeXt50 (32 X 4d) pre trained on the ImageNet Dataset as the encoder. This encoder is implemented and can be imported from the torchvision library."],"metadata":{"id":"75OFwM5C4z0_"}},{"cell_type":"code","source":["## Firstly, we have to load the pretrained model and explore what will be the dimensions of tensors that would be outputed.\n","## All pre trained models in the Torchvision.models collections are trained on ImageNet only.\n","## the argument pretrained initializes those weights.\n","\n","model = models.resnext50_32x4d(pretrained = True)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cpQG6IlE4TeQ","executionInfo":{"status":"ok","timestamp":1656407881427,"user_tz":-330,"elapsed":843,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}},"outputId":"37cd6a18-26ff-444b-84e6-984ca3767596"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["model._modules.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GFkXJwoQljZP","executionInfo":{"status":"ok","timestamp":1656407881428,"user_tz":-330,"elapsed":18,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}},"outputId":"1c019ea1-2f89-4a71-8a7b-5b6b4e98d1ef"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['conv1', 'bn1', 'relu', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4', 'avgpool', 'fc'])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["## Now we have the names of the block which will return the outputs for the DRB routed skip connections through to the Decoders\n","skip_connections = ['maxpool','layer1', 'layer2', 'layer3', 'layer4']"],"metadata":{"id":"Z7_YrUdgm4St","executionInfo":{"status":"ok","timestamp":1656407881429,"user_tz":-330,"elapsed":6,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["## In this cell, we have stored the output sizes of each layer that will give a residual or a skip connection in a list \n","## This is neccessary for declaring the DRB and the AGA module\n","\n","x = torch.randn([5, 64, 112, 112])\n","a = model._modules[\"maxpool\"](x)\n","b = model._modules[\"layer1\"](a)\n","c = model._modules[\"layer2\"](b)\n","d = model._modules[\"layer3\"](c)\n","e = model._modules[\"layer4\"](d)\n","sizes_skip = [x.shape, a.shape, b.shape, c.shape, d.shape, e.shape]\n","del a, b, c, d, e, model "],"metadata":{"id":"dxI02nKvvuRb","executionInfo":{"status":"ok","timestamp":1656407882697,"user_tz":-330,"elapsed":1273,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["class DepthEstimationModel(Module) :\n","\n","    def __init__(self, skip_connections, sizes_skip) :\n","        super(DepthEstimationModel, self).__init__()\n","\n","        ## Constructor Instructions for the Encoder \n","        self.pretrained_encoder = models.resnext50_32x4d(pretrained = True).to(device)\n","\n","        ## The Pre-trained Encoder should not update its weigths during back propagation \n","        ## So, the following is done to stop the parameters of the encoder from training\n","        for param in self.pretrained_encoder.parameters() :\n","            param.requires_grad = False\n","\n","        self.skip_connections = skip_connections\n","        self.encoder_layers = self.pretrained_encoder._modules\n","\n","        ## Constructor Instructions for the Decoder\n","        self.ASPP = ASPP_Block(2048, 7, 7).to(device)\n","        self.skip_DRB = [DRB(sizes_skip[i + 1][1], sizes_skip[i + 1][2], sizes_skip[i + 1][3]).to(device) for i in range(5)]\n","        self.Decoders = list()\n","        for i in range(5, 0, -1) :\n","            curr_layer = Decoder_Layer(*sizes_skip[i][1 : ], *sizes_skip[i - 1][1 :]).to(device)\n","            self.Decoders.append(curr_layer)\n","\n","        ## Constructor of the Final Section \n","        self.Final_Section = nn.Sequential(\n","            nn.UpsamplingBilinear2d((224, 224)), \n","            nn.Conv2d(64, 1, (3, 3), (1, 1), padding = \"same\", bias = False),\n","            nn.Softmax()\n","        ).to(device)\n","\n","    def forward(self, x) :\n","        sc_idx = 0\n","        skip_outputs = list()\n","\n","        ## Forward Propagation of the Encoder and saving the skip connections\n","        for layer, key in enumerate(self.encoder_layers) :\n","            x = self.encoder_layers[key](x)\n","\n","            if key == self.skip_connections[sc_idx] :\n","                skip_outputs.append(x)\n","                sc_idx += 1\n","\n","                ## Since ResNext is designed to handle classification problems \n","                ## The Remaining layers are not needed in out use case\n","                if sc_idx >= 5 :\n","                    break \n","\n","            else : \n","                continue\n","\n","        ## Passing the Skip connections through the DRB Module\n","        for i, out in enumerate(skip_outputs) :\n","            skip_outputs[i] = self.skip_DRB[i](out)\n","\n","\n","        ## Developing the Decoder Module\n","        x = self.ASPP(x)\n","        skip_outputs = skip_outputs[::-1]\n","        for i, out in enumerate(skip_outputs) :\n","            x = self.Decoders[i](out, x)\n","\n","        del skip_outputs\n","\n","        ## The Final Output \n","        x = self.Final_Section(x)\n","        return x"],"metadata":{"id":"nxuQQJTUavKU","executionInfo":{"status":"ok","timestamp":1656407882698,"user_tz":-330,"elapsed":7,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["# **Building the Loss Functions**\n","\n","The paper describes a three term loss function, Virtual Normal Loss, Weighted Cross Entropy loss and finally the L2 loss. We are going to use the same code used by the authours of \"Enforcing Geometric Constraints of Virtual Normal for Depth Prediction\". The following is the GITHUB link of the Repository of the above mentioned code.\n","\n","https://github.com/YvanYin/VNL_Monocular_Depth_Prediction/blob/master/lib/models/VNL_loss.py"],"metadata":{"id":"n_D6arn5ucBP"}},{"cell_type":"code","source":["class VNL_Loss(nn.Module):\n","    \"\"\"\n","    Virtual Normal Loss Function.\n","    \"\"\"\n","    def __init__(self, focal_x, focal_y, input_size,\n","                 delta_cos=0.867, delta_diff_x=0.01,\n","                 delta_diff_y=0.01, delta_diff_z=0.01,\n","                 delta_z=0.0001, sample_ratio=0.15):\n","        super(VNL_Loss, self).__init__()\n","        self.fx = torch.tensor([focal_x], dtype=torch.float32).cuda()\n","        self.fy = torch.tensor([focal_y], dtype=torch.float32).cuda()\n","        self.input_size = input_size\n","        self.u0 = torch.tensor(input_size[1] // 2, dtype=torch.float32).cuda()\n","        self.v0 = torch.tensor(input_size[0] // 2, dtype=torch.float32).cuda()\n","        self.init_image_coor()\n","        self.delta_cos = delta_cos\n","        self.delta_diff_x = delta_diff_x\n","        self.delta_diff_y = delta_diff_y\n","        self.delta_diff_z = delta_diff_z\n","        self.delta_z = delta_z\n","        self.sample_ratio = sample_ratio\n","\n","    def init_image_coor(self):\n","        x_row = np.arange(0, self.input_size[1])\n","        x = np.tile(x_row, (self.input_size[0], 1))\n","        x = x[np.newaxis, :, :]\n","        x = x.astype(np.float32)\n","        x = torch.from_numpy(x.copy()).cuda()\n","        self.u_u0 = x - self.u0\n","\n","        y_col = np.arange(0, self.input_size[0])  # y_col = np.arange(0, height)\n","        y = np.tile(y_col, (self.input_size[1], 1)).T\n","        y = y[np.newaxis, :, :]\n","        y = y.astype(np.float32)\n","        y = torch.from_numpy(y.copy()).cuda()\n","        self.v_v0 = y - self.v0\n","\n","    def transfer_xyz(self, depth):\n","        x = self.u_u0 * torch.abs(depth) / self.fx\n","        y = self.v_v0 * torch.abs(depth) / self.fy\n","        z = depth\n","        pw = torch.cat([x, y, z], 1).permute(0, 2, 3, 1) # [b, h, w, c]\n","        return pw\n","\n","    def select_index(self):\n","        valid_width = self.input_size[1]\n","        valid_height = self.input_size[0]\n","        num = valid_width * valid_height\n","        p1 = np.random.choice(num, int(num * self.sample_ratio), replace=True)\n","        np.random.shuffle(p1)\n","        p2 = np.random.choice(num, int(num * self.sample_ratio), replace=True)\n","        np.random.shuffle(p2)\n","        p3 = np.random.choice(num, int(num * self.sample_ratio), replace=True)\n","        np.random.shuffle(p3)\n","\n","        p1_x = p1 % self.input_size[1]\n","        p1_y = (p1 / self.input_size[1]).astype(np.int)\n","\n","        p2_x = p2 % self.input_size[1]\n","        p2_y = (p2 / self.input_size[1]).astype(np.int)\n","\n","        p3_x = p3 % self.input_size[1]\n","        p3_y = (p3 / self.input_size[1]).astype(np.int)\n","        p123 = {'p1_x': p1_x, 'p1_y': p1_y, 'p2_x': p2_x, 'p2_y': p2_y, 'p3_x': p3_x, 'p3_y': p3_y}\n","        return p123\n","\n","    def form_pw_groups(self, p123, pw):\n","        \"\"\"\n","        Form 3D points groups, with 3 points in each grouup.\n","        :param p123: points index\n","        :param pw: 3D points\n","        :return:\n","        \"\"\"\n","        p1_x = p123['p1_x']\n","        p1_y = p123['p1_y']\n","        p2_x = p123['p2_x']\n","        p2_y = p123['p2_y']\n","        p3_x = p123['p3_x']\n","        p3_y = p123['p3_y']\n","\n","        pw1 = pw[:, p1_y, p1_x, :]\n","        pw2 = pw[:, p2_y, p2_x, :]\n","        pw3 = pw[:, p3_y, p3_x, :]\n","        # [B, N, 3(x,y,z), 3(p1,p2,p3)]\n","        pw_groups = torch.cat([pw1[:, :, :, np.newaxis], pw2[:, :, :, np.newaxis], pw3[:, :, :, np.newaxis]], 3)\n","        return pw_groups\n","\n","    def filter_mask(self, p123, gt_xyz, delta_cos=0.867,\n","                    delta_diff_x=0.005,\n","                    delta_diff_y=0.005,\n","                    delta_diff_z=0.005):\n","        pw = self.form_pw_groups(p123, gt_xyz)\n","        pw12 = pw[:, :, :, 1] - pw[:, :, :, 0]\n","        pw13 = pw[:, :, :, 2] - pw[:, :, :, 0]\n","        pw23 = pw[:, :, :, 2] - pw[:, :, :, 1]\n","        ###ignore linear\n","        pw_diff = torch.cat([pw12[:, :, :, np.newaxis], pw13[:, :, :, np.newaxis], pw23[:, :, :, np.newaxis]],\n","                            3)  # [b, n, 3, 3]\n","        m_batchsize, groups, coords, index = pw_diff.shape\n","        proj_query = pw_diff.view(m_batchsize * groups, -1, index).permute(0, 2, 1)  # (B* X CX(3)) [bn, 3(p123), 3(xyz)]\n","        proj_key = pw_diff.view(m_batchsize * groups, -1, index)  # B X  (3)*C [bn, 3(xyz), 3(p123)]\n","        q_norm = proj_query.norm(2, dim=2)\n","        nm = torch.bmm(q_norm.view(m_batchsize * groups, index, 1), q_norm.view(m_batchsize * groups, 1, index)) #[]\n","        energy = torch.bmm(proj_query, proj_key)  # transpose check [bn, 3(p123), 3(p123)]\n","        norm_energy = energy / (nm + 1e-8)\n","        norm_energy = norm_energy.view(m_batchsize * groups, -1)\n","        mask_cos = torch.sum((norm_energy > delta_cos) + (norm_energy < -delta_cos), 1) > 3  # igonre\n","        mask_cos = mask_cos.view(m_batchsize, groups)\n","        ##ignore padding and invilid depth\n","        mask_pad = torch.sum(pw[:, :, 2, :] > self.delta_z, 2) == 3\n","\n","        ###ignore near\n","        mask_x = torch.sum(torch.abs(pw_diff[:, :, 0, :]) < delta_diff_x, 2) > 0\n","        mask_y = torch.sum(torch.abs(pw_diff[:, :, 1, :]) < delta_diff_y, 2) > 0\n","        mask_z = torch.sum(torch.abs(pw_diff[:, :, 2, :]) < delta_diff_z, 2) > 0\n","\n","        mask_ignore = (mask_x & mask_y & mask_z) | mask_cos\n","        mask_near = ~mask_ignore\n","        mask = mask_pad & mask_near\n","\n","        return mask, pw\n","\n","    def select_points_groups(self, gt_depth, pred_depth):\n","        pw_gt = self.transfer_xyz(gt_depth)\n","        pw_pred = self.transfer_xyz(pred_depth)\n","        B, C, H, W = gt_depth.shape\n","        p123 = self.select_index()\n","        # mask:[b, n], pw_groups_gt: [b, n, 3(x,y,z), 3(p1,p2,p3)]\n","        mask, pw_groups_gt = self.filter_mask(p123, pw_gt,\n","                                              delta_cos=0.867,\n","                                              delta_diff_x=0.005,\n","                                              delta_diff_y=0.005,\n","                                              delta_diff_z=0.005)\n","\n","        # [b, n, 3, 3]\n","        pw_groups_pred = self.form_pw_groups(p123, pw_pred)\n","        pw_groups_pred[pw_groups_pred[:, :, 2, :] == 0] = 0.0001\n","        mask_broadcast = mask.repeat(1, 9).reshape(B, 3, 3, -1).permute(0, 3, 1, 2)\n","        pw_groups_pred_not_ignore = pw_groups_pred[mask_broadcast].reshape(1, -1, 3, 3)\n","        pw_groups_gt_not_ignore = pw_groups_gt[mask_broadcast].reshape(1, -1, 3, 3)\n","\n","        return pw_groups_gt_not_ignore, pw_groups_pred_not_ignore\n","\n","    def forward(self, gt_depth, pred_depth, select=True):\n","        \"\"\"\n","        Virtual normal loss.\n","        :param pred_depth: predicted depth map, [B,W,H,C]\n","        :param data: target label, ground truth depth, [B, W, H, C], padding region [padding_up, padding_down]\n","        :return:\n","        \"\"\"\n","        gt_points, dt_points = self.select_points_groups(gt_depth, pred_depth)\n","\n","        gt_p12 = gt_points[:, :, :, 1] - gt_points[:, :, :, 0]\n","        gt_p13 = gt_points[:, :, :, 2] - gt_points[:, :, :, 0]\n","        dt_p12 = dt_points[:, :, :, 1] - dt_points[:, :, :, 0]\n","        dt_p13 = dt_points[:, :, :, 2] - dt_points[:, :, :, 0]\n","\n","        gt_normal = torch.cross(gt_p12, gt_p13, dim=2)\n","        dt_normal = torch.cross(dt_p12, dt_p13, dim=2)\n","        dt_norm = torch.norm(dt_normal, 2, dim=2, keepdim=True)\n","        gt_norm = torch.norm(gt_normal, 2, dim=2, keepdim=True)\n","        dt_mask = dt_norm == 0.0\n","        gt_mask = gt_norm == 0.0\n","        dt_mask = dt_mask.to(torch.float32)\n","        gt_mask = gt_mask.to(torch.float32)\n","        dt_mask *= 0.01\n","        gt_mask *= 0.01\n","        gt_norm = gt_norm + gt_mask\n","        dt_norm = dt_norm + dt_mask\n","        gt_normal = gt_normal / gt_norm\n","        dt_normal = dt_normal / dt_norm\n","        loss = torch.abs(gt_normal - dt_normal)\n","        loss = torch.sum(torch.sum(loss, dim=2), dim=0)\n","        if select:\n","            loss, indices = torch.sort(loss, dim=0, descending=False)\n","            loss = loss[int(loss.size(0) * 0.25):]\n","        loss = torch.mean(loss)\n","        return loss"],"metadata":{"id":"GizBi97nwU2I","executionInfo":{"status":"ok","timestamp":1656407883407,"user_tz":-330,"elapsed":715,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["Combining the Loss Functions and developing a class "],"metadata":{"id":"oj9C2ou_3QQi"}},{"cell_type":"code","source":["class TrainingLoss(Module) :\n","\n","    def __init__(self, alpha = 6, beta = 25) :\n","\n","        super(TrainingLoss, self).__init__()\n","        self.alpha = alpha \n","        self.beta = beta\n","        self.VirtualNormalLoss = VNL_Loss(1.0, 1.0, (224, 224))\n","        self.CrossEntropyLoss = nn.CrossEntropyLoss()\n","        self.l2_loss = nn.MSELoss()\n","\n","    def forward(self, pred, target) :\n","        a = self.VirtualNormalLoss(pred, target).to(device)\n","        b = self.CrossEntropyLoss(pred, target).to(device)\n","        c = self.l2_loss(pred, target).to(device)\n","\n","        out = b + self.alpha * a + self.beta * c\n","        del a, b, c\n","        return out \n"],"metadata":{"id":"0InaFdbpys1j","executionInfo":{"status":"ok","timestamp":1656407883408,"user_tz":-330,"elapsed":14,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["# **Defining the Main Image Augmentations and specific training functions**"],"metadata":{"id":"v0C239MN_jae"}},{"cell_type":"code","source":["## Image Augmentation using Albumentations\n","\n","train_transform = A.Compose(\n","    [\n","        A.Resize(Image_height, Image_width),\n","        A.Rotate(limit = 35, p = 1.0),\n","        A.HorizontalFlip(p = 0.5),\n","        A.VerticalFlip(p = 0.1),\n","        A.Normalize(\n","            mean = [0.0, 0.0, 0.0],\n","            std = [1.0, 1.0, 1.0],\n","            max_pixel_value = 255.0\n","        ),\n","        ToTensorV2(),  \n","    ]\n",")\n","\n","val_transforms = A.Compose(\n","    [\n","        A.Resize(Image_height, Image_width), \n","        A.Normalize(\n","            mean = [0.0, 0.0, 0.0],\n","            std = [1.0, 1.0, 1.0],\n","            max_pixel_value = 255.0\n","        ),\n","        ToTensorV2(),\n","    ]\n",")"],"metadata":{"id":"fxMQADaA-cRf","executionInfo":{"status":"ok","timestamp":1656407883409,"user_tz":-330,"elapsed":14,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["## Defining the train Function\n","\n","def train_fn(loader, model, optimizer, loss_fn, scaler) :\n","    loop = tqdm(loader)\n","\n","    for batch_idx, (data, targets) in enumerate(loop) :\n","        data = data.to(device)\n","        targets = targets.float().squeeze().unsqueeze(1).to(device)\n","\n","        ## Forward\n","        with torch.cuda.amp.autocast() :\n","            predictions = model(data)\n","            loss = loss_fn(predictions, targets)\n","\n","        ## Backwards \n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        ## Update the tqdm loader\n","        loop.set_postfix(loss = loss.item())\n","\n"],"metadata":{"id":"6EKees24BtDA","executionInfo":{"status":"ok","timestamp":1656407883410,"user_tz":-330,"elapsed":14,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["class TrainingModel() :\n","\n","    def __init__(\n","        self, skip_connections, sizes_skip, learning_rate, num_epochs, alpha = 6, beta = 25, device = \"cuda\"\n","        ) :\n","\n","        self.model = DepthEstimationModel(skip_connections, sizes_skip).to(device)\n","        self.loss_fn = TrainingLoss(alpha, beta).to(device)\n","        self.optimizer = O.Adam(self.model.parameters(), lr = learning_rate)\n","        self.scheduler = LinearLR(self.optimizer, 0.33, 1)\n","\n","        self.train_loader, self.val_loader = get_loaders(\n","            train_df, val_df, batch_size, train_transform, val_transforms,\n","            num_workers, pin_memory \n","        )\n","        self.num_epochs = num_epochs\n","        self.learning_rate = learning_rate\n","        self.device = device\n","\n","    def Training(self, load_model) :\n","\n","        if load_model :\n","            load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), self.model)\n","\n","        #check_accuracy(self.val_loader, self.model, device)\n","        scaler = torch.cuda.amp.GradScaler()\n","\n","        for epoch in range(self.num_epochs) :\n","            train_fn(self.train_loader, self.model, self.optimizer, self.loss_fn, scaler)\n","            self.scheduler.step()\n","\n","            ## Saving the Model\n","            checkpoint = {\n","                \"state_dict\" : self.model.state_dict(), \n","                \"optimizer\" : self.optimizer.state_dict()   \n","            }\n","\n","            save_checkpoint(checkpoint)\n","\n","            ## Checking the Accuracy \n","            #check_accuracy(self.val_loader, self.model, self.device) "],"metadata":{"id":"Dse7BObbEQgb","executionInfo":{"status":"ok","timestamp":1656407883411,"user_tz":-330,"elapsed":14,"user":{"displayName":"2K20-MC-133-Shobhit Bansal","userId":"11000123525734611630"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["## Final Training Call\n","Trainer = TrainingModel(skip_connections, sizes_skip, learning_rate, num_epochs, device = device)\n","Trainer.Training(load_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nblXAOss_ciw","outputId":"fcb83406-ddf2-4843-8fc2-c736ca3729dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  3%|▎         | 78/2535 [12:12<5:09:24,  7.56s/it, loss=2.11e+5]"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"p_suRZJwGYIE"},"execution_count":null,"outputs":[]}]}