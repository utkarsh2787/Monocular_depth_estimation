{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lightweight Monocular Depth Estimation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lightweight Monocular Depth Estimation on Edge Devices**\n",
        "\n",
        "This paper was published most recently at the IEE Xplore platform that proposed a novice LightWeight Method of Depth Estimation on Edge Devices. They proposed a completely new artchitecture derived from MobileNet, reduced latecy using Pruning Methods and then finally optimized it further by GPU overhead Scheduling."
      ],
      "metadata": {
        "id": "Aa2-U5xfMfWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup and Imports**"
      ],
      "metadata": {
        "id": "fP69yBM-Nnx4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgdfgNyzMdmr"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import sys \n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.nn import relu6 \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model \n",
        "from tensorflow.keras import optimizers as OPT\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras.utils import Sequence \n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import cv2 \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        " \n",
        "tf.random.set_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the Dataset**\n",
        "\n",
        "The Datasets have already been downloaded and annotated in an external hard disk. There is also a .csv file that has the path to each RGB image andd their corresponding depth maps."
      ],
      "metadata": {
        "id": "tx_SWCf4PHLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_root = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "df = pd.read_csv(\"nyu2_test.csv\", names = [\"image\", \"depth\"])\n",
        "df"
      ],
      "metadata": {
        "id": "jV_ZNQzyO8_x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "a78529c0-5dd5-4ee3-e530-d9c29fc2733e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               image                           depth\n",
              "0    data/nyu2_test/00000_colors.png  data/nyu2_test/00000_depth.png\n",
              "1    data/nyu2_test/00001_colors.png  data/nyu2_test/00001_depth.png\n",
              "2    data/nyu2_test/00008_colors.png  data/nyu2_test/00008_depth.png\n",
              "3    data/nyu2_test/00013_colors.png  data/nyu2_test/00013_depth.png\n",
              "4    data/nyu2_test/00014_colors.png  data/nyu2_test/00014_depth.png\n",
              "..                               ...                             ...\n",
              "649  data/nyu2_test/01444_colors.png  data/nyu2_test/01444_depth.png\n",
              "650  data/nyu2_test/01445_colors.png  data/nyu2_test/01445_depth.png\n",
              "651  data/nyu2_test/01446_colors.png  data/nyu2_test/01446_depth.png\n",
              "652  data/nyu2_test/01447_colors.png  data/nyu2_test/01447_depth.png\n",
              "653  data/nyu2_test/01448_colors.png  data/nyu2_test/01448_depth.png\n",
              "\n",
              "[654 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ffb1a3d3-b13b-4793-9cac-c9682196dad3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>depth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/nyu2_test/00000_colors.png</td>\n",
              "      <td>data/nyu2_test/00000_depth.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/nyu2_test/00001_colors.png</td>\n",
              "      <td>data/nyu2_test/00001_depth.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/nyu2_test/00008_colors.png</td>\n",
              "      <td>data/nyu2_test/00008_depth.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/nyu2_test/00013_colors.png</td>\n",
              "      <td>data/nyu2_test/00013_depth.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/nyu2_test/00014_colors.png</td>\n",
              "      <td>data/nyu2_test/00014_depth.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649</th>\n",
              "      <td>data/nyu2_test/01444_colors.png</td>\n",
              "      <td>data/nyu2_test/01444_depth.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650</th>\n",
              "      <td>data/nyu2_test/01445_colors.png</td>\n",
              "      <td>data/nyu2_test/01445_depth.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651</th>\n",
              "      <td>data/nyu2_test/01446_colors.png</td>\n",
              "      <td>data/nyu2_test/01446_depth.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>652</th>\n",
              "      <td>data/nyu2_test/01447_colors.png</td>\n",
              "      <td>data/nyu2_test/01447_depth.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>653</th>\n",
              "      <td>data/nyu2_test/01448_colors.png</td>\n",
              "      <td>data/nyu2_test/01448_depth.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>654 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ffb1a3d3-b13b-4793-9cac-c9682196dad3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ffb1a3d3-b13b-4793-9cac-c9682196dad3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ffb1a3d3-b13b-4793-9cac-c9682196dad3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"image\"] = df[\"image\"].apply(lambda x : path_root + str(x))\n",
        "df[\"depth\"] = df[\"depth\"].apply(lambda x : path_root + str(x))\n",
        "df"
      ],
      "metadata": {
        "id": "Jitth2WyEhAG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "460ff2a0-2a99-40a7-f819-6f12e0a72d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 image  \\\n",
              "0    /content/drive/MyDrive/Colab Notebooks/data/ny...   \n",
              "1    /content/drive/MyDrive/Colab Notebooks/data/ny...   \n",
              "2    /content/drive/MyDrive/Colab Notebooks/data/ny...   \n",
              "3    /content/drive/MyDrive/Colab Notebooks/data/ny...   \n",
              "4    /content/drive/MyDrive/Colab Notebooks/data/ny...   \n",
              "..                                                 ...   \n",
              "649  /content/drive/MyDrive/Colab Notebooks/data/ny...   \n",
              "650  /content/drive/MyDrive/Colab Notebooks/data/ny...   \n",
              "651  /content/drive/MyDrive/Colab Notebooks/data/ny...   \n",
              "652  /content/drive/MyDrive/Colab Notebooks/data/ny...   \n",
              "653  /content/drive/MyDrive/Colab Notebooks/data/ny...   \n",
              "\n",
              "                                                 depth  \n",
              "0    /content/drive/MyDrive/Colab Notebooks/data/ny...  \n",
              "1    /content/drive/MyDrive/Colab Notebooks/data/ny...  \n",
              "2    /content/drive/MyDrive/Colab Notebooks/data/ny...  \n",
              "3    /content/drive/MyDrive/Colab Notebooks/data/ny...  \n",
              "4    /content/drive/MyDrive/Colab Notebooks/data/ny...  \n",
              "..                                                 ...  \n",
              "649  /content/drive/MyDrive/Colab Notebooks/data/ny...  \n",
              "650  /content/drive/MyDrive/Colab Notebooks/data/ny...  \n",
              "651  /content/drive/MyDrive/Colab Notebooks/data/ny...  \n",
              "652  /content/drive/MyDrive/Colab Notebooks/data/ny...  \n",
              "653  /content/drive/MyDrive/Colab Notebooks/data/ny...  \n",
              "\n",
              "[654 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d7b22ee-cd78-4ecc-8166-416123079ad5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>depth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>652</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>653</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/data/ny...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>654 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d7b22ee-cd78-4ecc-8166-416123079ad5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0d7b22ee-cd78-4ecc-8166-416123079ad5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0d7b22ee-cd78-4ecc-8166-416123079ad5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building a Data Pipeline**\n",
        "\n",
        "1. The Data pipeline takes a dataframe containing the path to the RGB image and the corresponding depth map \n",
        "2. It reads and resizes the RGB images. \n",
        "3. It reads the depth map image and resizes it.\n",
        "4. It then finally returns the RGB images and their corresponding depth map one batch at a time."
      ],
      "metadata": {
        "id": "NiqK5KaEPe_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPipeline(Sequence) :\n",
        "\n",
        "    def __init__(self, data, batch_size, dim = (256, 256), n_channels = 3, shuffle = True) :\n",
        "        \"\"\"\n",
        "        Initialisation\n",
        "        \"\"\"\n",
        "\n",
        "        self.data = data ## dataframe containing paths to images\n",
        "        self.indices = self.data.index.tolist() ## List of indices of images\n",
        "        self.dim = dim ## dimensions of each image\n",
        "        self.n_channels = n_channels ## Number of Channels for input image \n",
        "        self.batch_size = batch_size ## Number of images in each batch\n",
        "        self.shuffle = shuffle ## Boolean to indicate Shuffling \n",
        "        self.on_epoch_end() \n",
        "        ## Above is a property of the parent class tensoflow.keras.utils.sequence\n",
        "        ## which is used if we wish to modify the dataset at each epoch end.\n",
        "\n",
        "    def __len__(self) :\n",
        "        \"\"\"\n",
        "        Returns number of batches in the sequence\n",
        "        \"\"\"\n",
        "\n",
        "        return int(np.ceil(len(self.data) / self.batch_size))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index) :\n",
        "        \"\"\"\n",
        "        Returning the next batch of images and corresponding depth maps \n",
        "        \"\"\"\n",
        "\n",
        "        ## In case there are not enough data points to fill a batch.\n",
        "        if (index + 1) * self.batch_size > len(self.indices) :\n",
        "            self.batch_size = len(self.indices) - index * self.batch_size\n",
        "\n",
        "        ## Generating one batch of data \n",
        "            ## Firstly generating the indices of the data \n",
        "        batch_index = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "\n",
        "        ## Finding list of ids \n",
        "        batch = [self.indices[k] for k in batch_index]\n",
        "        x, y = self.Data_Generation(batch)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def on_epoch_end(self) :\n",
        "\n",
        "        \"\"\"\n",
        "        Updating the indices after each apoch\n",
        "        \"\"\"\n",
        "\n",
        "        self.index = np.arange(len(self.indices))\n",
        "        if self.shuffle is True :\n",
        "            np.random.shuffle(self.index)\n",
        "\n",
        "    def load(self, img_path, depth_path) :\n",
        "\n",
        "        \"\"\"\n",
        "        Loads the input RGB image and corresponding depth map \n",
        "        \"\"\"\n",
        "\n",
        "        image_ = cv2.imread(img_path)\n",
        "        ## OpenCV reads colour images in BGR format by default \n",
        "        ## So conversion is neccessary \n",
        "        image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
        "        image_ = cv2.resize(image_, self.dim)\n",
        "        image_ = tf.image.convert_image_dtype(image_, tf.float32)\n",
        "\n",
        "        depth_ = cv2.imread(depth_path)\n",
        "        depth_ = cv2.cvtColor(depth_, cv2.COLOR_BGR2GRAY)\n",
        "        depth_.resize(self.dim[0], self.dim[1], 1)\n",
        "        depth_ = tf.image.convert_image_dtype(depth_, tf.float32)\n",
        "\n",
        "        return image_, depth_\n",
        "\n",
        "\n",
        "    def Data_Generation(self, batch) :\n",
        "\n",
        "        x = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size, *self.dim, 1))\n",
        "\n",
        "        for i, batch_id in enumerate(batch) :\n",
        "            x[i, ], y[i, ] = self.load(\n",
        "                self.data[\"image\"][batch_id], \n",
        "                self.data[\"depth\"][batch_id]\n",
        "            )\n",
        "\n",
        "        return x, y\n",
        "    \n"
      ],
      "metadata": {
        "id": "-TB6iViRPdfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Declaring Hyperparameters**"
      ],
      "metadata": {
        "id": "mV5T385txtia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HEIGHT =  224\n",
        "WIDTH = 224\n",
        "LR = 0.02\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 10"
      ],
      "metadata": {
        "id": "N_L9nySluJjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building the Model**\n",
        "\n",
        "1. Here, we will generate some special classes to implement the Encoder that is the MobileNetV2 without the classifier, which will need to declare an inherited class of tensorflow.keras.layers.Layer to design the residual and downsizing block.\n",
        "2. We will also define a class to implement the UpSampConv layer declared in the paper. "
      ],
      "metadata": {
        "id": "Nr8T-qLXyD07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualMobileNet(layers.Layer) :\n",
        "\n",
        "    def __init__(self, units, padding = \"same\", **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "        self.convStart = layers.Conv2D(units, (1, 1), (1, 1), padding = padding)\n",
        "        self.DepthConv = layers.DepthwiseConv2D((3, 3), (1, 1), padding = padding)\n",
        "        self.convEnd = layers.Conv2D(units, (1, 1), (1, 1), padding = padding, activation = 'linear')\n",
        "        self.conc = layers.Concatenate()\n",
        "        self.bn2start = layers.BatchNormalization()\n",
        "        self.bn2depth = layers.BatchNormalization()\n",
        "        self.bn2end = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, input_tensor) :\n",
        "        d = self.convStart(input_tensor)\n",
        "        x = self.bn2start(d)\n",
        "        x = relu6(x)\n",
        "\n",
        "        x = self.DepthConv(x)\n",
        "        x = self.bn2depth(x)\n",
        "        x = relu6(x)\n",
        "\n",
        "        x = self.convEnd(x)\n",
        "        x = self.bn2end(x)\n",
        "\n",
        "        x = self.conc([x, input_tensor])\n",
        "        return x\n",
        "\n",
        "class DownScalingMobileNet(layers.Layer) :\n",
        "\n",
        "    def __init__(self, units, padding = \"same\", **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "        self.convStart = layers.Conv2D(units, (1, 1), (1, 1), padding = padding)\n",
        "        self.DepthConv = layers.DepthwiseConv2D((3, 3), (2, 2), padding = padding)\n",
        "        self.convEnd = layers.Conv2D(units, (1, 1), (1, 1), padding = padding, activation = 'linear')\n",
        "        self.bn2start = layers.BatchNormalization()\n",
        "        self.bn2depth = layers.BatchNormalization()\n",
        "        self.bn2end = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, input_tensor) :\n",
        "        d = self.convStart(input_tensor)\n",
        "        x = self.bn2start(d)\n",
        "        x = relu6(x)\n",
        "\n",
        "        x = self.DepthConv(x)\n",
        "        x = self.bn2depth(x)\n",
        "        x = relu6(x)\n",
        "\n",
        "        x = self.convEnd(x)\n",
        "        x = self.bn2depth(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class UpsampConv(layers.Layer) :\n",
        "\n",
        "    def __init__(self, units, padding = \"same\", **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "        self.convA = layers.Conv2D(units, (1, 1), (1, 1), padding = padding)\n",
        "        self.convB = layers.Conv2D(units, (3, 3), (1, 1), padding = padding)\n",
        "        self.convC = layers.DepthwiseConv2D((5, 5), (1, 1), padding = padding)\n",
        "        self.bn2A = layers.BatchNormalization()\n",
        "        self.bn2B = layers.BatchNormalization()\n",
        "        self.bn2C = layers.BatchNormalization()\n",
        "        self.conc = layers.Concatenate()\n",
        "\n",
        "    def call(self, input_tensor) :\n",
        "        d = self.convA(input_tensor)\n",
        "        x = self.bn2A(d)\n",
        "\n",
        "        y = self.convB(x)\n",
        "        y = self.bn2B(y)\n",
        "\n",
        "        z = self.convC(x)\n",
        "        z = self.bn2C(x)\n",
        "\n",
        "        x = tf.identity(x)\n",
        "        res = self.conc([x, y, z])\n",
        "        return res \n",
        "\n",
        "class UpSamplingBlock(layers.Layer):\n",
        "\n",
        "    def __init__(self, units, height, width, padding = \"same\", **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "        self.upsamp = UpsampConv(units, padding = padding)\n",
        "        self.nearest = layers.Resizing(height, width, \"nearest\")\n",
        "        self.bn = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, input_tensor) :\n",
        "        x = self.upsamp(input_tensor)\n",
        "        x = self.nearest(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "VoW6H-U6yaV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining the Loss functions**"
      ],
      "metadata": {
        "id": "uoah8say_dKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DepthEstimation(Model) :\n",
        "\n",
        "    def __init__(self) :\n",
        "        super().__init__()\n",
        "        self.ssim_loss_weight = 0.85\n",
        "        self.l1_loss_weight = 0.1\n",
        "        self.edge_loss_weight = 0.9\n",
        "        self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "        ## Encoder\n",
        "        self.convA = layers.Conv2D(32, (1, 1), (2, 2), padding = \"same\")\n",
        "        self.Layer1 = ResidualMobileNet(16)\n",
        "        self.Layer2 = DownScalingMobileNet(24)\n",
        "        self.Layer3 = DownScalingMobileNet(32)\n",
        "        self.Layer4 = DownScalingMobileNet(64)\n",
        "        self.Layer5 = ResidualMobileNet(96)\n",
        "        self.Layer6 = DownScalingMobileNet(160)\n",
        "        self.Layer7 = ResidualMobileNet(320)\n",
        "        self.convB = layers.Conv2D(1280, (1, 1), (1, 1))\n",
        "\n",
        "        ## Decoder\n",
        "        self.Layer8 = UpSamplingBlock(640, 14, 14)\n",
        "        self.Layer9 = UpSamplingBlock(320, 28, 28)\n",
        "        self.Layer10 = UpSamplingBlock(160, 56, 56)\n",
        "        self.Layer11 = UpSamplingBlock(80, 112, 112)\n",
        "        self.Layer12 = UpSamplingBlock(40, 224, 224)\n",
        "        self.convC = layers.Conv2D(1, (1, 1), (1, 1))\n",
        "\n",
        "\n",
        "    def calculate_loss(self, target, pred) :\n",
        "        # Edges\n",
        "        dy_true, dx_true = tf.image.image_gradients(target)\n",
        "        dy_pred, dx_pred = tf.image.image_gradients(pred)\n",
        "        weights_x = tf.exp(tf.reduce_mean(tf.abs(dx_true)))\n",
        "        weights_y = tf.exp(tf.reduce_mean(tf.abs(dy_true)))\n",
        "\n",
        "        # Depth smoothness\n",
        "        smoothness_x = dx_pred * weights_x\n",
        "        smoothness_y = dy_pred * weights_y\n",
        "\n",
        "        depth_smoothness_loss = tf.reduce_mean(abs(smoothness_x)) + tf.reduce_mean(\n",
        "            abs(smoothness_y)\n",
        "        )\n",
        "\n",
        "        # Structural similarity (SSIM) index\n",
        "        ssim_loss = tf.reduce_mean(\n",
        "            1\n",
        "            - tf.image.ssim(\n",
        "                target, pred, max_val=WIDTH, filter_size=7, k1=0.01 ** 2, k2=0.03 ** 2\n",
        "            )\n",
        "        )\n",
        "        # Point-wise depth\n",
        "        l1_loss = tf.reduce_mean(tf.abs(target - pred))\n",
        "\n",
        "        loss = (\n",
        "            (self.ssim_loss_weight * ssim_loss)\n",
        "            + (self.l1_loss_weight * l1_loss)\n",
        "            + (self.edge_loss_weight * depth_smoothness_loss)\n",
        "        )\n",
        "\n",
        "        return loss\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_metric]\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        input, target = batch_data\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred = self(input, training=True)\n",
        "            loss = self.calculate_loss(target, pred)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\n",
        "            \"loss\": self.loss_metric.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, batch_data):\n",
        "        input, target = batch_data\n",
        "\n",
        "        pred = self(input, training=False)\n",
        "        loss = self.calculate_loss(target, pred)\n",
        "\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\n",
        "            \"loss\": self.loss_metric.result(),\n",
        "        }\n",
        "\n",
        "\n",
        "    def call(self, input_tensor) :\n",
        "        \n",
        "        ## Calling the Encoder Layers\n",
        "        x = self.convA(input_tensor)\n",
        "        x = self.Layer1(x)\n",
        "        x = self.Layer2(x)\n",
        "        x = self.Layer3(x)\n",
        "        x = self.Layer4(x)\n",
        "        x = self.Layer5(x)\n",
        "        x = self.Layer6(x)\n",
        "        x = self.Layer7(x)\n",
        "        x = self.convB(x)\n",
        "\n",
        "        ## Calling the Deocder Layers\n",
        "        x = self.Layer8(x)\n",
        "        x = self.Layer9(x)\n",
        "        x = self.Layer10(x)\n",
        "        x = self.Layer11(x)\n",
        "        x = self.Layer12(x)\n",
        "        x = self.convC(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "i-eHjU8R_ORh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = OPT.Adam(\n",
        "    learning_rate=LR,\n",
        "    amsgrad=False,\n",
        ")\n",
        "model = DepthEstimation()\n",
        "# Define the loss function\n",
        "cross_entropy = losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=\"none\"\n",
        ")\n",
        "# Compile the model\n",
        "model.compile(optimizer, loss=cross_entropy)\n",
        "train_loader = DataPipeline(\n",
        "     data = df[:100].reset_index(drop=\"true\"), batch_size=BATCH_SIZE, dim=(HEIGHT, WIDTH)\n",
        ")\n",
        "validation_loader = DataPipeline(\n",
        "     data = df[100: 110].reset_index(drop=\"true\"), batch_size=BATCH_SIZE, dim=(HEIGHT, WIDTH)\n",
        ")\n",
        "model.fit(\n",
        "    train_loader, epochs=EPOCHS, validation_data = validation_loader\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwLPBX5vAZAL",
        "outputId": "244b2326-c7b0-421c-f9eb-37c017c29b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['depth_estimation_2/up_sampling_block_6/upsamp_conv_6/depthwise_conv2d_26/depthwise_kernel:0', 'depth_estimation_2/up_sampling_block_6/upsamp_conv_6/depthwise_conv2d_26/bias:0', 'depth_estimation_2/up_sampling_block_7/upsamp_conv_7/depthwise_conv2d_27/depthwise_kernel:0', 'depth_estimation_2/up_sampling_block_7/upsamp_conv_7/depthwise_conv2d_27/bias:0', 'depth_estimation_2/up_sampling_block_8/upsamp_conv_8/depthwise_conv2d_28/depthwise_kernel:0', 'depth_estimation_2/up_sampling_block_8/upsamp_conv_8/depthwise_conv2d_28/bias:0', 'depth_estimation_2/up_sampling_block_9/upsamp_conv_9/depthwise_conv2d_29/depthwise_kernel:0', 'depth_estimation_2/up_sampling_block_9/upsamp_conv_9/depthwise_conv2d_29/bias:0', 'depth_estimation_2/up_sampling_block_10/upsamp_conv_10/depthwise_conv2d_30/depthwise_kernel:0', 'depth_estimation_2/up_sampling_block_10/upsamp_conv_10/depthwise_conv2d_30/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['depth_estimation_2/up_sampling_block_6/upsamp_conv_6/depthwise_conv2d_26/depthwise_kernel:0', 'depth_estimation_2/up_sampling_block_6/upsamp_conv_6/depthwise_conv2d_26/bias:0', 'depth_estimation_2/up_sampling_block_7/upsamp_conv_7/depthwise_conv2d_27/depthwise_kernel:0', 'depth_estimation_2/up_sampling_block_7/upsamp_conv_7/depthwise_conv2d_27/bias:0', 'depth_estimation_2/up_sampling_block_8/upsamp_conv_8/depthwise_conv2d_28/depthwise_kernel:0', 'depth_estimation_2/up_sampling_block_8/upsamp_conv_8/depthwise_conv2d_28/bias:0', 'depth_estimation_2/up_sampling_block_9/upsamp_conv_9/depthwise_conv2d_29/depthwise_kernel:0', 'depth_estimation_2/up_sampling_block_9/upsamp_conv_9/depthwise_conv2d_29/bias:0', 'depth_estimation_2/up_sampling_block_10/upsamp_conv_10/depthwise_conv2d_30/depthwise_kernel:0', 'depth_estimation_2/up_sampling_block_10/upsamp_conv_10/depthwise_conv2d_30/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            " 1/10 [==>...........................] - ETA: 3:20 - loss: 1.2348"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCP8P4lFDVn9",
        "outputId": "d0403045-5435-4519-f9e5-cbd00145e946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UpmeI2k3DW1r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}